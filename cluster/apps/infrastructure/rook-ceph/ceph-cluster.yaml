# -- Cluster ceph.conf override
configOverride: |
  [global]
  osd_pool_default_size = 1
  mon_warn_on_pool_no_redundancy = false
  bdev_flock_retry = 20
  bluefs_buffered_io = false
  mon_data_avail_warn = 10

monitoring:
  enabled: true
  createPrometheusRules: true

cephClusterSpec:
  cephVersion:
    image: quay.io/ceph/ceph:v18.2
    allowUnsupported: false
  dataDirHostPath: /var/lib/rook
  mon:
    count: 1
    allowMultiplePerNode: true
  mgr:
    count: 1
    allowMultiplePerNode: true
    modules:
      - name: pg_autoscaler
        enabled: true
      - name: rook
        enabled: true
  dashboard:
    enabled: true
    ssl: false
  crashCollector:
    disable: false
    daysToRetain: 7
  # enable log collector, daemons will log on files and rotate
  logCollector:
    enabled: true
    periodicity: weekly # one of: hourly, daily, weekly, monthly
    maxLogSize: 500M # SUFFIX may be 'M' or 'G'. Must be at least 1
  cleanupPolicy:
    confirmation: ""

  resources:
    mgr:
      requests:
        cpu: "125m"
        memory: "549M"
      limits:
        memory: "1219M"
    mon:
      requests:
        cpu: "49m"
        memory: "477M"
      limits:
        memory: "1059M"
    osd:
      requests:
        cpu: "100m"
        memory: "2678M"
      limits:
        memory: "5944M"
        cpu: 300m
    mgr-sidecar:
      requests:
        cpu: "49m"
        memory: "94M"
      limits:
        memory: "208M"
    crashcollector:
      requests:
        cpu: "15m"
        memory: "64M"
      limits:
        memory: "64M"
    logcollector:
      requests:
        cpu: "100m"
        memory: "100M"
      limits:
        memory: "1G"
    cleanup:
      requests:
        cpu: "250m"
        memory: "100M"
      limits:
        memory: "1G"

  # priority classes to apply to ceph resources
  priorityClassNames:
    mon: system-node-critical
    osd: system-node-critical
    mgr: system-cluster-critical

  storage:
    useAllNodes: false
    useAllDevices: false
    config:
      osdsPerDevice: "1"
    nodes:
      - name: parche
        devices:
          - name: "/dev/disk/by-id/ata-WDC_WD40EFRX-68WT0N0_WD-WCC4E6ZHVFSP"
          - name: "/dev/disk/by-id/ata-WDC_WD101EFBX-68B0AN0_VHGW8LNM"
          - name: "/dev/disk/by-id/ata-Samsung_SSD_850_EVO_250GB_S2R5NB0HA05691Y"
          - name: "/dev/disk/by-id/nvme-Samsung_SSD_980_PRO_1TB_S5P2NU0WA19259N"
          - name: "/dev/disk/by-id/ata-WDC_WDS400T1R0A-68A4W0_234411800211"

  disruptionManagement:
    managePodBudgets: true
  healthCheck:
    daemonHealth:
      mon:
        interval: 45s
        timeout: 600s

ingress:
  # -- Enable an ingress for the ceph-dashboard
  dashboard:
    annotations:
      cert-manager.io/cluster-issuer: 'ca-issuer'
      gethomepage.dev/enabled: "true"
      gethomepage.dev/icon: ceph.png
      gethomepage.dev/group: Monitoring
      gethomepage.dev/name: "Ceph Dashboard"
    host:
      name: ceph-dashboard.seawolf.hermleigh.home
      path: /
    tls:
    - hosts:
        - ceph-dashboard.seawolf.hermleigh.home
        - ceph-dashboard.seawolf
      secretName: ceph-dashboard-tls
    # Note: Only one of ingress class annotation or the `ingressClassName:` can be used at a time
    # to set the ingress class
    ingressClassName: nginx

cephBlockPools:
  - name: k8s-block-ssd
    spec:
      failureDomain: osd
      replicated:
        size: 1
      deviceClass: ssd
    storageClass:
      enabled: true
      name: ceph-block-ssd
      isDefault: false
      reclaimPolicy: Retain
      allowVolumeExpansion: true
      parameters: &csi-block-params
        imageFormat: "2"
        imageFeatures: layering
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
        csi.storage.k8s.io/fstype: ext4

  - name: k8s-block-hdd
    spec:
      failureDomain: osd
      replicated:
        size: 1
      deviceClass: hdd
    storageClass:
      enabled: true
      name: ceph-block-hdd
      isDefault: false
      reclaimPolicy: Retain
      allowVolumeExpansion: true
      parameters: *csi-block-params

  - name: libvirt-pool
    spec:
      failureDomain: osd
      replicated:
        size: 1
      deviceClass: ssd
    storageClass:
      enabled: false

cephFileSystems:
  - name: k8s-hdd-fs
    spec:
      metadataPool:
        replicated:
          size: 1
        deviceClass: nvme
      dataPools:
        - name: data0
          failureDomain: osd
          replicated:
            size: 1
            requireSafeReplicaSize: false
          deviceClass: hdd
      metadataServer:
        activeCount: 1
        activeStandby: true
        resources:
          requests:
            cpu: "35m"
            memory: "128M"
          limits:
            memory: "512M"
    storageClass:
      enabled: true
      isDefault: false
      name: ceph-fs-hdd
      pool: data0
      reclaimPolicy: Retain
      allowVolumeExpansion: true
      mountOptions: []
      parameters: &csi-fs-params
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
        csi.storage.k8s.io/fstype: ext4

  - name: k8s-ssd-fs
    spec:
      metadataPool:
        replicated:
          size: 1
        deviceClass: nvme
      dataPools:
        - name: data1
          failureDomain: osd
          replicated:
            size: 1
            requireSafeReplicaSize: false
          deviceClass: ssd
      metadataServer:
        activeCount: 1
        activeStandby: true
        resources:
          requests:
            cpu: "35m"
            memory: "128M"
          limits:
            memory: "4096M"
    storageClass:
      enabled: true
      isDefault: true
      name: ceph-fs-ssd
      pool: data1
      reclaimPolicy: Retain
      allowVolumeExpansion: true
      mountOptions: []
      parameters: *csi-fs-params

cephBlockPoolsVolumeSnapshotClass:
  enabled: false

cephObjectStores:
  - name: s3
    spec:
      metadataPool:
        replicated:
          size: 1
        deviceClass: nvme
      dataPool:
        replicated:
          size: 1
        deviceClass: ssd
      preservePoolsOnDelete: false
      gateway:
        port: 80
        resources:
          requests:
            cpu: 100m
            memory: 128M
          limits:
            memory: 1Gi
        instances: 1
    storageClass:
      enabled: true
      name: s3
      reclaimPolicy: Retain
    ingress:
      # Enable an ingress for the ceph-objectstore
      enabled: true
      annotations:
        cert-manager.io/cluster-issuer: 'ca-issuer'
      host:
        name: s3.hermleigh.home
        path: /
      tls:
      - hosts:
          - s3.hermleigh.home
          - s3
        secretName: ceph-objectstore-tls
      ingressClassName: nginx
