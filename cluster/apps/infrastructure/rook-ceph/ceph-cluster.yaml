# -- Cluster ceph.conf override
configOverride: |
  [global]
  osd_pool_default_size = 1
  mon_warn_on_pool_no_redundancy = false
  bdev_flock_retry = 20
  bluefs_buffered_io = false
  mon_data_avail_warn = 10

monitoring:
  enabled: true
  createPrometheusRules: true

cephClusterSpec:
  cephVersion:
    image: quay.io/ceph/ceph:v18.2
    allowUnsupported: false
  dataDirHostPath: /var/lib/rook
  mon:
    count: 1
    allowMultiplePerNode: true
  mgr:
    count: 1
    allowMultiplePerNode: true
    modules:
      - name: pg_autoscaler
        enabled: true
      - name: rook
        enabled: true
  dashboard:
    enabled: true
    ssl: false
  crashCollector:
    disable: false
    daysToRetain: 7
  # enable log collector, daemons will log on files and rotate
  logCollector:
    enabled: true
    periodicity: weekly # one of: hourly, daily, weekly, monthly
    maxLogSize: 500M # SUFFIX may be 'M' or 'G'. Must be at least 1
  cleanupPolicy:
    confirmation: ""

  # priority classes to apply to ceph resources
  priorityClassNames:
    mon: system-node-critical
    osd: system-node-critical
    mgr: system-cluster-critical

  storage:
    useAllNodes: false
    useAllDevices: false
    config:
      osdsPerDevice: "1"
    nodes:
      - name: parche
        devices:
          - name: "/dev/disk/by-id/ata-WDC_WD40EFRX-68WT0N0_WD-WCC4E6ZHVFSP"
          - name: "/dev/disk/by-id/ata-WDC_WD101EFBX-68B0AN0_VHGW8LNM"
          - name: "/dev/disk/by-id/ata-Samsung_SSD_850_EVO_250GB_S2R5NB0HA05691Y"
          - name: "/dev/disk/by-id/nvme-Samsung_SSD_980_PRO_1TB_S5P2NU0WA19259N"
          - name: "/dev/disk/by-id/ata-WDC_WDS400T1R0A-68A4W0_234411800211"

  disruptionManagement:
    managePodBudgets: true
  healthCheck:
    daemonHealth:
      mon:
        interval: 45s
        timeout: 600s

ingress:
  # -- Enable an ingress for the ceph-dashboard
  dashboard:
    annotations:
      cert-manager.io/common-name: &host ceph-dashboard.seawolf.hermleigh.home
      cert-manager.io/subject-countries: US
      cert-manager.io/subject-organizations: "HERMLEIGH.HOME"
      cert-manager.io/subject-organizationalunits: "Ceph Cluster"
      cert-manager.io/private-key-algorithm: ECDSA
      cert-manager.io/private-key-size: "384"
      gethomepage.dev/enabled: "true"
      gethomepage.dev/icon: ceph.png
      gethomepage.dev/group: Monitoring
      gethomepage.dev/name: "Ceph Dashboard"
    host:
      name: *host
      path: /
    tls:
    - hosts:
        - *host
        - ceph-dashboard.seawolf
      secretName: ceph-dashboard-tls
    # Note: Only one of ingress class annotation or the `ingressClassName:` can be used at a time
    # to set the ingress class
    ingressClassName: nginx

cephBlockPools:
  - name: k8s-block-ssd
    spec:
      failureDomain: osd
      replicated:
        size: 1
      deviceClass: ssd
    storageClass:
      enabled: true
      name: ceph-block-ssd
      isDefault: false
      reclaimPolicy: Retain
      allowVolumeExpansion: true
      parameters: &csi-block-params
        imageFormat: "2"
        imageFeatures: layering
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
        csi.storage.k8s.io/fstype: ext4

  - name: k8s-block-hdd
    spec:
      failureDomain: osd
      replicated:
        size: 1
      deviceClass: hdd
    storageClass:
      enabled: true
      name: ceph-block-hdd
      isDefault: false
      reclaimPolicy: Retain
      allowVolumeExpansion: true
      parameters: *csi-block-params

  - name: libvirt-pool
    spec:
      failureDomain: osd
      replicated:
        size: 1
      deviceClass: ssd
    storageClass:
      enabled: false

cephFileSystems:
  - name: k8s-hdd-fs
    spec:
      metadataPool:
        replicated:
          size: 1
        deviceClass: nvme
      dataPools:
        - name: data0
          failureDomain: osd
          replicated:
            size: 1
            requireSafeReplicaSize: false
          deviceClass: hdd
      metadataServer:
        activeCount: 1
        activeStandby: true
        resources:
          requests:
            cpu: "35m"
            memory: "128M"
          limits:
            memory: "512M"
    storageClass:
      enabled: true
      isDefault: false
      name: ceph-fs-hdd
      pool: data0
      reclaimPolicy: Retain
      allowVolumeExpansion: true
      mountOptions: []
      parameters: &csi-fs-params
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
        csi.storage.k8s.io/fstype: ext4

  - name: k8s-ssd-fs
    spec:
      metadataPool:
        replicated:
          size: 1
        deviceClass: nvme
      dataPools:
        - name: data1
          failureDomain: osd
          replicated:
            size: 1
            requireSafeReplicaSize: false
          deviceClass: ssd
      metadataServer:
        activeCount: 1
        activeStandby: true
        resources:
          requests:
            cpu: "35m"
            memory: "128M"
          limits:
            memory: "4096M"
    storageClass:
      enabled: true
      isDefault: true
      name: ceph-fs-ssd
      pool: data1
      reclaimPolicy: Retain
      allowVolumeExpansion: true
      mountOptions: []
      parameters: *csi-fs-params

cephBlockPoolsVolumeSnapshotClass:
  enabled: false

cephObjectStores:
  - name: rgw
    spec:
      metadataPool:
        replicated:
          size: 1
        deviceClass: nvme
      dataPool:
        replicated:
          size: 1
        deviceClass: hdd
      preservePoolsOnDelete: false
      gateway:
        port: 80
        resources:
          requests:
            cpu: 100m
            memory: 128M
          limits:
            memory: 1Gi
        instances: 1
        dashboardEnabled: true
    storageClass:
      enabled: true
      name: ceph-rgw
      reclaimPolicy: Retain
    ingress:
      # Enable an ingress for the ceph-objectstore
      enabled: true
      annotations:
        cert-manager.io/common-name: &host rgw.hermleigh.home
        cert-manager.io/subject-countries: US
        cert-manager.io/subject-organizations: "HERMLEIGH.HOME"
        cert-manager.io/subject-organizationalunits: "Ceph Cluster"
        cert-manager.io/private-key-algorithm: ECDSA
        cert-manager.io/private-key-size: "384"
      host:
        name: *host
        path: /
      tls:
      - hosts:
          - *host
          - rgw
        secretName: ceph-objectstore-tls
      ingressClassName: nginx
